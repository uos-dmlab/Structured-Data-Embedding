{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import pickle\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from itertools import chain\n",
    "import statistics as stats\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "# Fasttext 모델 임포트\n",
    "from gensim.models import fasttext\n",
    "\n",
    "ft_model = fasttext.load_facebook_model(\"C:\\관계형테이블임베딩\\cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험에 사용할 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험에 사용할 데이터 불러오기\n",
    "\n",
    "file_list = os.listdir('C:\\관계형테이블임베딩\\데이터셋_20240520_N-Join-Pair')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_file_list = []\n",
    "data = []\n",
    "for i in tqdm(range(len(file_list))):\n",
    "    try:\n",
    "        table = pd.read_csv('C:\\관계형테이블임베딩\\데이터셋_20240520_N-Join-Pair/{}'.format(file_list[i]), encoding = 'ISO-8859-1')\n",
    "    except:\n",
    "        pop_file_list.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_file_list.sort(reverse=True)\n",
    "for i in pop_file_list:\n",
    "    file_list.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(file_list)):\n",
    "    data.append(pd.read_csv('C:\\관계형테이블임베딩\\데이터셋_20240520_N-Join-Pair/{}'.format(file_list[i]), encoding = 'ISO-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(file_list), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_list=[]\n",
    "for i in range(len(data)):\n",
    "    cat_columns = data[i].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    data[i] = data[i][cat_columns]\n",
    "    data[i] = data[i].applymap(str)\n",
    "    if (data[i].shape[0]) == 0 or (data[i].shape[1]) == 0:\n",
    "        pop_list.append(i)\n",
    "pop_list.sort(reverse=True)\n",
    "\n",
    "for i in pop_list:\n",
    "    data.pop(i)\n",
    "    file_list.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_list)):\n",
    "    print(i, \" \", file_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result_vector/table2vecFastText_file_list.pkl\",\"wb\") as f:\n",
    "    pickle.dump(file_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Join-Pair 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def word_extract(tab):\n",
    "    column_heading = tab.columns.tolist()\n",
    "    column_value = tab.values.tolist()\n",
    "    column_value = list(chain(*column_value))\n",
    "    return list(set(column_heading + column_value))\n",
    "\n",
    "def late_avg(list1, list2):\n",
    "    late = 0\n",
    "    for vec1 in list1:\n",
    "        for vec2 in list2:\n",
    "            late += cos_sim(vec1,vec2)\n",
    "    if len(list1) * len(list2) > 0 :\n",
    "        late_avg = late / (len(list1) * len(list2))\n",
    "    else:\n",
    "        return 0\n",
    "    return late_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_list = []\n",
    "#tab_word_list = []\n",
    "for i in tqdm(range(len(file_list))):\n",
    "    tab_list.append(file_list[i])\n",
    "    #tab_word_list.append(word_extract(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_wordvec_list= []\n",
    "for word_list in tab_word_list:\n",
    "    temp_list = []\n",
    "    for word in word_list:\n",
    "        try :\n",
    "            temp_list.append(ft_model.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "    tab_wordvec_list.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_dict = {}\n",
    "for i in range(len(tab_list)):\n",
    "        tab_dict[tab_list[i]] = tab_wordvec_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(file_list))):\n",
    "    if \"@\" in file_list[i] :\n",
    "        print(file_list[i])\n",
    "        \n",
    "        a,b,c = file_list[i].split(\" @ \")\n",
    "        a,b,c = a.split(\"^\")[0], b.split(\"^\")[0], c.split(\"^\")[0]\n",
    "        print(f'조인된 테이블과 {a} 테이블간의 코사인유사도 : {late_avg(tab_dict[file_list[i]], tab_dict[a])} \\n조인된 테이블과 {b} 테이블간의 코사인유사도 : {late_avg(tab_dict[file_list[i]], tab_dict[b])} \\n조인된 테이블과 {c} 테이블간의 코사인유사도 : {late_avg(tab_dict[file_list[i]], tab_dict[c])} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result_vector/table2vecFastText.pkl\",\"rb\") as f:\n",
    "    tab_wordvec_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(lst):\n",
    "    min_val = min(lst)\n",
    "    max_val = max(lst)\n",
    "    range_val = max_val - min_val\n",
    "    if range_val == 0:  # 모든 값이 동일한 경우, 모든 값을 0으로 설정\n",
    "        return [0 for _ in lst]\n",
    "    else:\n",
    "        return [(item - min_val) / range_val for item in lst]\n",
    "\n",
    "def standardize(lst):\n",
    "    mean = stats.mean(lst)\n",
    "    stdev = stats.stdev(lst)\n",
    "    return [(i - mean) / stdev for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_values(a, b, c):\n",
    "    invalid_values = {\"airlines.csv\", \"List of Unicorns in the World.csv\", \"loan-train.csv\"}\n",
    "    if a in invalid_values or b in invalid_values or c in invalid_values:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_list = []\n",
    "join_tab_list = []\n",
    "cos_dict = {}\n",
    "for i in tqdm(range(len(file_list))):\n",
    "    if \"@\" in file_list[i] :\n",
    "        a,b,c = file_list[i].split(\" @ \")\n",
    "        a,b,c = a.split(\"^\")[0], b.split(\"^\")[0], c.split(\"^\")[0]\n",
    "        if check_values(a,b,c):\n",
    "            join_tab_list.append(file_list[i])\n",
    "            #cos_list.append(late_avg(tab_dict[file_list[i]], tab_dict[a]) + late_avg(tab_dict[file_list[i]], tab_dict[b]) + late_avg(tab_dict[file_list[i]], tab_dict[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result_vector/table2vecFastText_coslist.pkl\",\"rb\") as f:\n",
    "    cos_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_list = standardize(cos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_temp = []\n",
    "for i in range(len(cos_list)):\n",
    "    if cos_list[i]>2 or cos_list[i]<-2:\n",
    "        pop_temp.append(i)\n",
    "pop_temp.sort(reverse=True)\n",
    "print(pop_temp)\n",
    "for i in pop_temp:\n",
    "    join_tab_list.pop(i)\n",
    "    cos_list.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_list = normalize(cos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(join_tab_list)):\n",
    "        \n",
    "        cos_dict[join_tab_list[i]] = cos_list[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 계산 및 TF-IDF overlapping을 활용한 N-Join-Pair 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extract_value(tab):\n",
    "    #column_heading = tab.columns.tolist()\n",
    "    column_value = tab.values.tolist()\n",
    "    column_value = list(chain(*column_value))\n",
    "    return column_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORD_LIST = []\n",
    "for tab in data:\n",
    "    ALL_WORD_LIST.append(word_extract_value(tab))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result_vector/table2vecFastText_ALL_WORD_LIST.pkl\",\"rb\") as f:\n",
    "    ALL_WORD_LIST = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORD_LIST_DB = []\n",
    "for word_list in ALL_WORD_LIST:\n",
    "    ALL_WORD_LIST_DB.append([i.replace(' ','') for i in word_list])\n",
    "print(ALL_WORD_LIST_DB[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generator_to_file(generator, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for data in generator:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "# 제너레이터를 사용하여 데이터를 청크 단위로 저장\n",
    "save_generator_to_file(ALL_WORD_LIST_DB, \"./result_vector/table2vecFastText_ALL_WORD_LIST_DB.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALL_WORD_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 리스트를 문자열로 변환\n",
    "corpus = [' '.join(word_list) for word_list in ALL_WORD_LIST_DB]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#딕셔너리 생성\n",
    "ALL_TF_IDF_DICT= {}\n",
    "# TF-IDF 값이 높은 상위 10개의 단어 추출\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for i, word_list in enumerate(ALL_WORD_LIST_DB):\n",
    "    tfidf_values = X.toarray()[i]\n",
    "    if '@' in file_list[i]:\n",
    "        top_indices = tfidf_values.argsort()[-30:]\n",
    "    else:\n",
    "        top_indices = tfidf_values.argsort()[-10:]\n",
    "    top_words = [feature_names[j] for j in top_indices]\n",
    "    ALL_TF_IDF_DICT[file_list[i]] = top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result_vector/table2vecFastText_ALL_TF_IDF_DICT.pkl\",\"rb\") as f:\n",
    "    ALL_TF_IDF_DICT = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_OV(joined_tab):\n",
    "    tab_col1,tab_col2,tab_col3 = joined_tab.split(\" @ \")\n",
    "    tab1,col1 = tab_col1.split(\"^\")\n",
    "    tab2,col2 = tab_col2.split(\"^\")\n",
    "    tab3,col3 = tab_col3.split(\"^\")\n",
    "    col3 = col3[:-4]\n",
    "    \n",
    "    sum = 0\n",
    "    for word in (ALL_TF_IDF_DICT[tab1] + ALL_TF_IDF_DICT[tab2] + ALL_TF_IDF_DICT[tab3]):\n",
    "        if word in ALL_TF_IDF_DICT[joined_tab]:\n",
    "            sum+=1\n",
    "    \n",
    "    \n",
    "\n",
    "    return sum / len(ALL_TF_IDF_DICT[joined_tab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['agriculture-value-added-per-worker-wdi new.csv^country',\n",
    " '2022.csv^Country',\n",
    " '2016.csv^Country',\n",
    " 'IPL Matches 2008-2020.csv^city',\n",
    " '2019_nCoV_20200121_20200128.csv^Province/State',\n",
    " 'ingredients.csv^ing_id',\n",
    " 'medications.csv^Disease',\n",
    " 'IPL Matches 2008-2020.csv^player_of_match',\n",
    " '18_19_premier_league_Squad Possession.csv^Squad',\n",
    " 'matches.csv^team2',\n",
    " 'diets.csv^Disease',\n",
    " '2021.csv^Regional indicator',\n",
    " 'rugs.csv^HHA Concept 1',\n",
    " '2015.csv^Country',\n",
    " 'matches.csv^winner',\n",
    " '2019_world_happiness.csv^Country or region',\n",
    " 'loan-train.csv^Loan_ID',\n",
    " 'global_transport.csv^Country Code',\n",
    " 'match_info_data.csv^player_of_match',\n",
    " 'world-happiness-report-2021.csv^Regional indicator',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체적으로 상관계수 계산\n",
    "cos_value = []\n",
    "TF_IDF_value = []\n",
    "for q in query:\n",
    "    for key, value in cos_dict.items():\n",
    "        if key.split(\" @ \")[0] == q or key.split(\" @ \")[1] == q or key.split(\" @ \")[2] == q:\n",
    "            cos_value.append(value)\n",
    "            TF_IDF_value.append(TF_IDF_OV(key))\n",
    "correlation, p_value = pearsonr(cos_value, TF_IDF_value)\n",
    "print(f\"20개의 Query 테이블을 소스 테이블로 사용하는 모든 조인 융합 테이블의 cosine 유사도, TF-IDF-OV의 상관계수 : {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각각 상관계수 계산\n",
    "corr_list = []\n",
    "for q in query:\n",
    "    cos_value = []\n",
    "    TF_IDF_value = []\n",
    "    for key, value in cos_dict.items():\n",
    "        if key.split(\" @ \")[0] == q or key.split(\" @ \")[1] == q or key.split(\" @ \")[2] == q:\n",
    "            cos_value.append(value)\n",
    "            TF_IDF_value.append(TF_IDF_OV(key))\n",
    "    correlation_matrix = np.corrcoef(cos_value, TF_IDF_value)\n",
    "    corr_list.append(correlation_matrix[0][1])\n",
    "    print(q, correlation_matrix[0][1])\n",
    "# NaN 값 제거\n",
    "data_without_nan = [x for x in corr_list if not np.isnan(x)]\n",
    "\n",
    "# 평균 계산\n",
    "average = sum(data_without_nan) / len(data_without_nan)\n",
    "print(\"20개 Query 테이블 각각에 대해 상관계수를 구하고 이를 평균낸 값 :\", average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    print(np.average(globals()['tier{}'.format(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#구간 5개\n",
    "for i in range(1,6):\n",
    "    globals()['tier{}'.format(i)] = []\n",
    "    \n",
    "for i in range(10):\n",
    "    print(f\"\\n{i+1}티어 조인\\n\")\n",
    "    \n",
    "    if i==9:\n",
    "        for key, value in cos_dict.items():\n",
    "            if (10-(i+1))/10 <= value <= (10-i)/10:\n",
    "                try:\n",
    "                    tier10.append(TF_IDF_OV(key))\n",
    "                    print(key)\n",
    "                except Exception as e:\n",
    "                    # 그 외 모든 예외 처리\n",
    "                    print(\"An error occurred:\", e)\n",
    "    else:\n",
    "\n",
    "        for key, value in cos_dict.items():\n",
    "            if (10-(i+1))/10 < value <= (10-i)/10:\n",
    "                try:\n",
    "                    globals()['tier{}'.format(i+1)].append(TF_IDF_OV(key))\n",
    "                    print(key)\n",
    "                except Exception as e:\n",
    "                    # 그 외 모든 예외 처리\n",
    "                    print(\"An error occurred:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
